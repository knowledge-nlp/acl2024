<!DOCTYPE HTML>

<html>
	<head>
		<title>Welcome to KnowledgeNLP-ACL’24!</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">
					<!-- Logo -->
<!--						<img src="images/isail_logo.png" alt="" /><span><h2>Welcome to <a href="index.html" id="logo">iSAIL</a> Lab !</h2></span>-->

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="cfp.html">Calls</a></li>
								<li><a href="schedule.html">Schedule</a></li>
								<li class="current"><a href="keynote.html">Keynote</a></li>
								<li><a href="publications.html">Accepted Papers</a></li>
								<li><a href="organization.html">Organization</a></li>
							</ul>
						</nav>

				</div>
				<section id="banner">
					<!-- <header>
						<h2>Welcome to workshop on Knowledge Augmented Methods for NLP (KnowledgeNLP-ACL’24)!</h2>
						<a href="#" class="button">Learn More</a>
					</header> -->
				</section>



				<section class="wrapper style1">
					<div class="container">
						<div id="content">	
					
					<div style="overflow: hidden;">
					   <div id="A" style="float:left; width: 25%;">
						  <img src="images/minlie_huang.jpg" alt="" width="220px" style="margin:0px 50px">
					   </div>
					   <div id="B" style="float: left; width: 75%;">
						  <div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							<b> <u>Prof. Minlie Huang</u> </b><br>
						  </div>
						
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Professor at Department of Computer Science, Institute for Artificial Intelligence, Tsinghua University<br>
							</div>

							<div>
								<b>Title: </b> TBA <br>
								<b>Time: </b> 9:10 - 9:50 am <br>
								<!-- <b>Abstract: </b> “Data alone is not enough.” This was the section heading in Pedro Dominguez’s 2012 seminal paper. I have been a believer in this for a long time. In our Semantic Search engine commercialized in 2000, also described in a patent, we complemented machine learning classifiers with a comprehensive WorldModel™ or knowledge bases (now referred to as knowledge graphs) for improved named entity and relationship extraction and semantic search. It was an early demonstration of the complementary nature of data-driven statistical learning (since replaced by neural networks) and knowledge-supported symbolic AI methods.  In this talk, I want to observe three important issues corresponding to the Why, What, and How of using knowledge in neuro-symbolic AI systems. While the transformer-based models have achieved tremendous success in many NLP tasks, the purse data-driven approach comes up short when we need NLU, where knowledge is key to understanding the language, as required for the explanation, safety, and supporting decision-making processes that must be followed (e.g., in clinical diagnosis). <br>
								I will share the following observations regarding the indispensable role of knowledge. <br>
								(a) WHY: while data-driven AI has done reasonably well for activities requiring focused and narrow-intellect activity that does not require a deeper understanding of content, whether natural language or other modalities such as classification, prediction, translation, and recommendation, for activities that rely on a deeper understanding of content and higher-intelligences, such as abstraction and analogy, and for activities that involve decision making and taking actions,  it is necessary to involve knowledge humans have gathered and codified. <br>
								(b) WHAT: the knowledge needed to support more demanding activities is multifaceted and comprehensive; it needs to cover multiple levels of abstraction. For example, humans utilize all these types of knowledge to understand a natural language: lexical, linguistic, common sense including a sense of time, geographic and sense of location, broad-based or world knowledge, domain/subject/task-specific knowledge, and more. Each of these types of knowledge is distinct in the way it is created, often through collective intelligence or collaborative activities, subject to processes that endow it with the quality needed for given tasks and require appropriate representational richness to capture the semantics needed.    <br>
								(c) HOW: Several ways to use knowledge to develop neuro-symbolic AI techniques have been proposed. I will describe knowledge-infusion strategies, ranging from shallow infusion using embedding techniques that sacrifice rich semantics in knowledge representation for limited gain, followed by semi-deep and deep knowledge-infusion techniques that retain semantics (such as those captured in expressive knowledge representation and model) to enhance the transformer models. The ultimate aim is to develop neuro-symbolic methods to address the limitations of large language models and to serve the needs that data-driven techniques fail to support. Further details: Advancing Neuro-symbolic AI with Deep Knowledge-infused Learning <br>
								<b>Bio: </b> Prof. Amit Sheth (Home Page, LinkedIn) is an Educator, Researcher, and Entrepreneur. He is the founding director of the university-wide AI Institute at the University of South Carolina. He is a Fellow of IEEE, AAAI, AAAS and ACM. He has (co-)founded four companies, including the first Semantic Search company in 1999 that pioneered technology similar to what is found today in Google Semantic Search and Knowledge Graph, ezDI which developed knowledge-infused clinical NLP/NLU, and Cognovi Labs at the intersection of emotion and AI. He is particularly proud of the success of his &gt;45 Ph.D. advisees and postdocs in academia, industry research, and entrepreneurs. <br> -->
							</div>
						</div>
					</div>
					<br>

					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/scott_yih.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Dr. Scott Wen-tau Yih </u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Research Scientist at Meta AI Research (previously known as FAIR)<br>
							</div>
  
						  	<div>
								<b>Title: </b> TBA <br>
								<b>Time: </b> 9:50 - 10:30 am <br>
								<!-- <b>Abstract: </b> While large-scale language models work incredibly well, it is expensive to train them, difficult to explain their predictions, and nearly impossible to keep them current over time. It is unclear when we can trust their predictions, and none of the current large language models can answer questions about current topics, such as COVID-19, since the corpora used for their training were created several years ago. To develop the next generation of general purpose language models with smaller, simpler, and much more efficient models, we believe information retrieval is a key component. When interacting with each other and with the world, humans tap into many different forms of knowledge, including world knowledge (e.g., commonsense, updated world facts, trending news) and user knowledge (e.g., conversational memory, social interactions, additional context such as location, etc.). To incorporate this capability in AI applications, information retrieval provides models access to (potentially large) collections of documents that can contain such knowledge. Specifically, we envision that the complete system consists of a small, core model that can easily access additional, task-related knowledge via retrieval, and perform comparably to the largest language models available today.
								In this talk, I will first give a research overview of retrieval-augmented language models. Then, I will share some of our recent work, including a general framework that improves any language models by adding a retrieval component, as well as a retrieval-augmented multimodal model that generates images and captions with better quality. Finally, I'll conclude the talk by discussing some of the lessons we learned and the problems we plan to address in the near future. <br>
								<b>Bio: </b> Scott Wen-tau Yih is a Research Scientist at Meta AI -- FAIR. His research interests include natural language processing, machine learning and information retrieval. Before joining Meta, Yih was a Principal Research Scientist at the Allen Institute for Artificial Intelligence (AI2), working on scientific question answering. Prior to that, Yih had spent 12 years at Microsoft Research, working on a variety of projects including email spam filtering, keyword extraction and search & ad relevance. His recent work focuses on continuous representations and neural network models for question answering and document retrieval. Yih received the best paper award from CoNLL’11, an outstanding paper award from ACL’15 and has served as program co-chairs (CEAS’09, CoNLL’14, EMNLP’21) and action/associated editors (TACL, JAIR) in recent years. He is also a co-presenter for several popular tutorials on topics including Semantic Role Labeling (NAACL’06, AAAI’07), Deep Learning for NLP (SLT’14, NAACL’15, IJCAI’16), Question Answering with Knowledge Base, Web and Beyond (NAACL’16, SIGIR’16), NLP for Precision Medicine (ACL’17) and Open-domain Question Answering (ACL’20). <br> -->
						  	</div>
						</div>
					</div>
					<br>



					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/yulia.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Prof. Yulia Tsvetkov </u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Associate Professor in the Paul G. Allen School of Computer Science & Engineering, University of Washington<br>
							</div>
  
						  	<div>
								<b>Title: </b> TBA <br>
								<b>Time: </b> 11:20 - 11:55 am <br>
								<!-- <b>Abstract: </b> While large-scale language models work incredibly well, it is expensive to train them, difficult to explain their predictions, and nearly impossible to keep them current over time. It is unclear when we can trust their predictions, and none of the current large language models can answer questions about current topics, such as COVID-19, since the corpora used for their training were created several years ago. To develop the next generation of general purpose language models with smaller, simpler, and much more efficient models, we believe information retrieval is a key component. When interacting with each other and with the world, humans tap into many different forms of knowledge, including world knowledge (e.g., commonsense, updated world facts, trending news) and user knowledge (e.g., conversational memory, social interactions, additional context such as location, etc.). To incorporate this capability in AI applications, information retrieval provides models access to (potentially large) collections of documents that can contain such knowledge. Specifically, we envision that the complete system consists of a small, core model that can easily access additional, task-related knowledge via retrieval, and perform comparably to the largest language models available today.
								In this talk, I will first give a research overview of retrieval-augmented language models. Then, I will share some of our recent work, including a general framework that improves any language models by adding a retrieval component, as well as a retrieval-augmented multimodal model that generates images and captions with better quality. Finally, I'll conclude the talk by discussing some of the lessons we learned and the problems we plan to address in the near future. <br>
								<b>Bio: </b> Scott Wen-tau Yih is a Research Scientist at Meta AI -- FAIR. His research interests include natural language processing, machine learning and information retrieval. Before joining Meta, Yih was a Principal Research Scientist at the Allen Institute for Artificial Intelligence (AI2), working on scientific question answering. Prior to that, Yih had spent 12 years at Microsoft Research, working on a variety of projects including email spam filtering, keyword extraction and search & ad relevance. His recent work focuses on continuous representations and neural network models for question answering and document retrieval. Yih received the best paper award from CoNLL’11, an outstanding paper award from ACL’15 and has served as program co-chairs (CEAS’09, CoNLL’14, EMNLP’21) and action/associated editors (TACL, JAIR) in recent years. He is also a co-presenter for several popular tutorials on topics including Semantic Role Labeling (NAACL’06, AAAI’07), Deep Learning for NLP (SLT’14, NAACL’15, IJCAI’16), Question Answering with Knowledge Base, Web and Beyond (NAACL’16, SIGIR’16), NLP for Precision Medicine (ACL’17) and Open-domain Question Answering (ACL’20). <br> -->
						  	</div>
						</div>
					</div>
					<br>




					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/greg.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Prof. Greg Durrett </u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Associate Professor at Department of Computer Science, UT Austin<br>
							</div>
  
						  	<div>
								<b>Title: </b> TBA <br>
								<b>Time: </b> 11:55 - 12:30 pm <br>
								<!-- <b>Abstract: </b> While large-scale language models work incredibly well, it is expensive to train them, difficult to explain their predictions, and nearly impossible to keep them current over time. It is unclear when we can trust their predictions, and none of the current large language models can answer questions about current topics, such as COVID-19, since the corpora used for their training were created several years ago. To develop the next generation of general purpose language models with smaller, simpler, and much more efficient models, we believe information retrieval is a key component. When interacting with each other and with the world, humans tap into many different forms of knowledge, including world knowledge (e.g., commonsense, updated world facts, trending news) and user knowledge (e.g., conversational memory, social interactions, additional context such as location, etc.). To incorporate this capability in AI applications, information retrieval provides models access to (potentially large) collections of documents that can contain such knowledge. Specifically, we envision that the complete system consists of a small, core model that can easily access additional, task-related knowledge via retrieval, and perform comparably to the largest language models available today.
								In this talk, I will first give a research overview of retrieval-augmented language models. Then, I will share some of our recent work, including a general framework that improves any language models by adding a retrieval component, as well as a retrieval-augmented multimodal model that generates images and captions with better quality. Finally, I'll conclude the talk by discussing some of the lessons we learned and the problems we plan to address in the near future. <br>
								<b>Bio: </b> Scott Wen-tau Yih is a Research Scientist at Meta AI -- FAIR. His research interests include natural language processing, machine learning and information retrieval. Before joining Meta, Yih was a Principal Research Scientist at the Allen Institute for Artificial Intelligence (AI2), working on scientific question answering. Prior to that, Yih had spent 12 years at Microsoft Research, working on a variety of projects including email spam filtering, keyword extraction and search & ad relevance. His recent work focuses on continuous representations and neural network models for question answering and document retrieval. Yih received the best paper award from CoNLL’11, an outstanding paper award from ACL’15 and has served as program co-chairs (CEAS’09, CoNLL’14, EMNLP’21) and action/associated editors (TACL, JAIR) in recent years. He is also a co-presenter for several popular tutorials on topics including Semantic Role Labeling (NAACL’06, AAAI’07), Deep Learning for NLP (SLT’14, NAACL’15, IJCAI’16), Question Answering with Knowledge Base, Web and Beyond (NAACL’16, SIGIR’16), NLP for Precision Medicine (ACL’17) and Open-domain Question Answering (ACL’20). <br> -->
						  	</div>
						</div>
					</div>
					<br>




					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/minjoon.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Prof. Minjoon Seo</u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Assistant Professor, Kim Jaechul Graduate School of AI, KAIST<br>
							</div>
  
						  	<div>
								<b>Title: </b> TBA <br>
								<b>Time: </b> 2:00 - 2:40 pm <br>
								<!-- <b>Abstract: </b> While large-scale language models work incredibly well, it is expensive to train them, difficult to explain their predictions, and nearly impossible to keep them current over time. It is unclear when we can trust their predictions, and none of the current large language models can answer questions about current topics, such as COVID-19, since the corpora used for their training were created several years ago. To develop the next generation of general purpose language models with smaller, simpler, and much more efficient models, we believe information retrieval is a key component. When interacting with each other and with the world, humans tap into many different forms of knowledge, including world knowledge (e.g., commonsense, updated world facts, trending news) and user knowledge (e.g., conversational memory, social interactions, additional context such as location, etc.). To incorporate this capability in AI applications, information retrieval provides models access to (potentially large) collections of documents that can contain such knowledge. Specifically, we envision that the complete system consists of a small, core model that can easily access additional, task-related knowledge via retrieval, and perform comparably to the largest language models available today.
								In this talk, I will first give a research overview of retrieval-augmented language models. Then, I will share some of our recent work, including a general framework that improves any language models by adding a retrieval component, as well as a retrieval-augmented multimodal model that generates images and captions with better quality. Finally, I'll conclude the talk by discussing some of the lessons we learned and the problems we plan to address in the near future. <br>
								<b>Bio: </b> Scott Wen-tau Yih is a Research Scientist at Meta AI -- FAIR. His research interests include natural language processing, machine learning and information retrieval. Before joining Meta, Yih was a Principal Research Scientist at the Allen Institute for Artificial Intelligence (AI2), working on scientific question answering. Prior to that, Yih had spent 12 years at Microsoft Research, working on a variety of projects including email spam filtering, keyword extraction and search & ad relevance. His recent work focuses on continuous representations and neural network models for question answering and document retrieval. Yih received the best paper award from CoNLL’11, an outstanding paper award from ACL’15 and has served as program co-chairs (CEAS’09, CoNLL’14, EMNLP’21) and action/associated editors (TACL, JAIR) in recent years. He is also a co-presenter for several popular tutorials on topics including Semantic Role Labeling (NAACL’06, AAAI’07), Deep Learning for NLP (SLT’14, NAACL’15, IJCAI’16), Question Answering with Knowledge Base, Web and Beyond (NAACL’16, SIGIR’16), NLP for Precision Medicine (ACL’17) and Open-domain Question Answering (ACL’20). <br> -->
						  	</div>
						</div>
					</div>
					<br>




					<div style="overflow: hidden;">
						<div id="A" style="float:left; width: 25%;">
							<img src="images/zhuosheng.jpg" alt="" width="220px" style="margin:0px 50px">
						</div>
						
						<div id="B" style="float: left; width: 75%;">
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 20px">
							  <b> <u>Prof. Zhuosheng Zhang </u> </b><br>
							</div>
						  
							<div style="font-family: Verdana, Geneva, sans-serif; font-size: 17px">
								Assistant Professor, Shanghai Jiao Tong University<br>
							</div>
  
						  	<div>
								<b>Title: </b> TBA <br>
								<b>Time: </b> 3:30 - 4:10 pm <br>
								<!-- <b>Abstract: </b> While large-scale language models work incredibly well, it is expensive to train them, difficult to explain their predictions, and nearly impossible to keep them current over time. It is unclear when we can trust their predictions, and none of the current large language models can answer questions about current topics, such as COVID-19, since the corpora used for their training were created several years ago. To develop the next generation of general purpose language models with smaller, simpler, and much more efficient models, we believe information retrieval is a key component. When interacting with each other and with the world, humans tap into many different forms of knowledge, including world knowledge (e.g., commonsense, updated world facts, trending news) and user knowledge (e.g., conversational memory, social interactions, additional context such as location, etc.). To incorporate this capability in AI applications, information retrieval provides models access to (potentially large) collections of documents that can contain such knowledge. Specifically, we envision that the complete system consists of a small, core model that can easily access additional, task-related knowledge via retrieval, and perform comparably to the largest language models available today.
								In this talk, I will first give a research overview of retrieval-augmented language models. Then, I will share some of our recent work, including a general framework that improves any language models by adding a retrieval component, as well as a retrieval-augmented multimodal model that generates images and captions with better quality. Finally, I'll conclude the talk by discussing some of the lessons we learned and the problems we plan to address in the near future. <br>
								<b>Bio: </b> Scott Wen-tau Yih is a Research Scientist at Meta AI -- FAIR. His research interests include natural language processing, machine learning and information retrieval. Before joining Meta, Yih was a Principal Research Scientist at the Allen Institute for Artificial Intelligence (AI2), working on scientific question answering. Prior to that, Yih had spent 12 years at Microsoft Research, working on a variety of projects including email spam filtering, keyword extraction and search & ad relevance. His recent work focuses on continuous representations and neural network models for question answering and document retrieval. Yih received the best paper award from CoNLL’11, an outstanding paper award from ACL’15 and has served as program co-chairs (CEAS’09, CoNLL’14, EMNLP’21) and action/associated editors (TACL, JAIR) in recent years. He is also a co-presenter for several popular tutorials on topics including Semantic Role Labeling (NAACL’06, AAAI’07), Deep Learning for NLP (SLT’14, NAACL’15, IJCAI’16), Question Answering with Knowledge Base, Web and Beyond (NAACL’16, SIGIR’16), NLP for Precision Medicine (ACL’17) and Open-domain Question Answering (ACL’20). <br> -->
						  	</div>
						</div>
					</div>
					<br>


					

				</section>


	
			<!-- Footer -->
				<!-- <div id="footer">
					<div class="container">
						<div class="row">

							<section class="col-6 col-12-narrower">
								<h3>Get In Touch</h3>
								<form>
									<div class="row gtr-50">
										<div class="col-6 col-12-mobilep">
											<input type="text" name="name" id="name" placeholder="Name" />
										</div>
										<div class="col-6 col-12-mobilep">
											<input type="email" name="email" id="email" placeholder="Email" />
										</div>
										<div class="col-12">
											<textarea name="message" id="message" placeholder="Message" rows="5"></textarea>
										</div>
										<div class="col-12">
											<ul class="actions">
												<li><input type="submit" class="button alt" value="Send Message" /></li>
											</ul>
										</div>
									</div>
								</form>
							</section>
						</div>
					</div> -->

					<!-- Icons -->
						<!-- <ul class="icons">
							<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="#" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="#" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
							<li><a href="#" class="icon brands fa-google-plus-g"><span class="label">Google+</span></a></li>
						</ul> -->

					<!-- Copyright -->
						<div class="copyright">
							<ul class="menu">
								<li>&copy;  All rights reserved</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>

				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
